% !Mode:: "TeX:UTF-8"

\chapter{相关技术及其核心原理}
\section{Lucene简介}

Lucene是Apache旗下的一个开源的全文索引工具包，用Java语言编写，它为应用中实现全文索引功能提供了基础\citeup{5}。Lucene提供了一套具有强大功能的API，可以比较方便的嵌入到各种应用中，实现应用中的全文索引、检索功能。在Java开发环境下，Lucene是一个开放源代码的工具包，就其自身来说，Lucene是最近这几年，比较受欢迎的开源java全文索引引擎工具包。

Doug Cutting是Lucene的作者，也是一名全文索引、检索方面的专家，曾经是Apple的Copland操作系统的成就之一“V-Twin”搜索引擎的主要研发人员，之后在担任Excite高级系统架构设计师一职，目前着手于Internet底层架构方面的研究。Lucene的目标是为各中小型应用程序提供全文检索功能。

\subsection{Lucene索引过程}
Lucene的索引过程如图\ref{image001}所示。
\pic[t]{Lucene索引过程}{}{image001}

1.提取文本:先从数据源里提取出纯文本格式的信息，让Lucene识别该文本信息并建立相对应的Lucene文档。Lucene没有提供有效的方法来从数据源中提取出有效的纯文本信息，这需要开发者自行实现提取纯文本信息的方法。

2.建立Lucene文档:使用提取出的纯文本信息来创建对应的Document实例。该\\Document实例包含若干个Field实例，Field实例中有两个比较重要的属性：域名和域值，域值的作用是保存纯文本信息中与该域对应的那部分。例如，对于一个公司的Document实例，应该包括公司名、拥有者、股东为域名的各种Field实例。每个Field实例的域值应该存储对应的内容，比如，某个公司名Field实例的域值为“XXXX有限公司”。

3.分析文档:分析(Analysis)是指将域(Field)文本转换成最基本的索引单元——项(term)的过程。在搜索的过程中，项决定了何种文档可以匹配查询条件。分析器封装了分析操作，通过执行这些操作，将纯文本信息转换成词汇单元，这些操作包括了：提取单词，去除标点符号、字母的规范化、去除常用词汇、词干还原，或词干归并。整个处理过程也叫做词汇单元化，从文本流提取出的文本块称为语汇单元（token）\citeup{7}。语汇单元和它的域名结合之后就形成了项。建立好Lucene的Document实例之后，就可以进行索引操作了，方法是调用IndexWriter对象的addDocument方法将数据传递给Lucene。对文本进行分析的过程会将域值处理成大量的语汇单元。

对输入的数据分析完成之后，会将分析结果写入索引文件中。Lucene会把输入数据以一种倒排索引（Inverted Index）的数据结构进行存储。那么倒排索引是什么呢？简单来说，倒排索引是经过优化后用来快速回答“哪些文档包含单词X？”而不是回答“这个文档包含了哪些单词？”现在几乎所有的Web搜索引擎核心采用的都是倒排索引技术。


在本系统中，数据源由FTP信息搜集模块搜集的FTP文件信息文本提供。

\subsection{倒排索引}
首先先介绍另一种搜索方式——顺序扫描，用于和倒排索引进行对比。

顺序扫描：当你要搜索包含某个字符串的文档时，遍历所有需要搜索的文档。对于每个文档，遍历文档的内容，如果该文档包含了所要搜索的字符串，那么该文档即为所需要搜索的文档，接下来遍历下一个文档，直到完成对所有文档的遍历。对于存储空间比较大的介质，如果要在其中找到一个包含某指定字符串的文档需要的时间会相对较长。这种搜索策略比较原始，适用于小数据量文档的搜索，方便快捷。但是对于大数据量的文档，这样的方法效率就相对较低。

倒排索引：由于从字符串到文档的映射是文档到字符串映射的逆向过程，所以将这种保存信息的索引方式称作倒排索引。

倒排索引保存的信息大致如下：假定文档集合里面有200篇文档，为了方便表示，为文档编号为1到200，得到图\ref{image004}的结构：

\pic[htbp]{倒排索引基本结构}{width=0.8\textwidth}{image004}

左边保存的是字符串信息，称为词典。
每个字符串都指向包含此字符串的文档链表，此文档链表叫做倒排表(Posting List)。
倒排索引保存的信息和需要搜索的信息是一致的，可以加快搜索的效率。
假设要寻找既包含字符串“solr”又包含字符串“lucene”的文档，需要如下几步：

1.	取出包含字符串“solr”的文档链表。

2.	取出包含字符串“lucene”的文档链表。

3.	通过合并链表，找出既包含“solr”又包含“lucene”的文件——5。

图\ref{image006}示意了上述过程。

\pic[htbp]{处理查询过程}{width=0.8\textwidth}{image006}

\subsection{词典实现原理}
进行查询时会使用到其提供的字典功能\citeup{8}，即根据给定的term找到该term所对应的倒排文档id列表等信息。关于字典的实现，图\ref{image004}给出了一种最简单的方案——排序数组，即term字典是一个已经按字母顺序排序好的数组，数组每一项存放着term和对应的倒排文档id列表。每次载入索引的时候只要将term数组载入内存，通过二分查找即可。这种方法查询时间复杂度为Log(N)，N指的是term数目，占用的空间大小是O(N\*str(term))。此的缺点是内存消耗较大，即需要完整存储每一个term，当term数目多达上千万时，占用的内存将十分大。

1、常用字典数据结构

很多数据结构都能完成字典功能，如表\ref{FSTtable1}所示:

\pictable[!htbp]{数据结构介绍}{width=0.8\textwidth}{FSTtable1}

2、FST原理简析

FST有两个优点：

1）空间占用小。通过对词典中单词前缀和后缀的重复利用，压缩了存储空间\citeup{13}；

2）查询速度快。O(len(str))的查询时间复杂度。

下面简单描述下FST的构造过程。我们对“cat”、 “deep”、 “do”、 “dog” 、\\
“dogs”这5个单词进行插入构建FST。

1）插入“cat”

插入cat，每个字母形成一条边，其中t边指向终点，如图\ref{FST1}所示：

\pic[ht]{插入cat}{width=0.6\textwidth}{FST1}

2）插入“deep”

与前一个单词“cat”进行最大前缀匹配，发现没有匹配则直接插入，P边指向终点，如图\ref{FST2}所示：

\pic[!ht]{插入“deep”}{width=0.6\textwidth}{FST2}

3）插入“do”

与前一个单词“deep”进行最大前缀匹配，发现是d，则在d边后增加新边o，o边指向终点，如图\ref{FST3}所示：

\pic[!ht]{插入“do”}{width=0.6\textwidth}{FST3}

4）插入“dog”

与前一个单词“do”进行最大前缀匹配，发现是do，则在o边后增加新边g，g边指向终点，如图\ref{FST4}所示：

\pic[!ht]{插入“dog”}{width=0.6\textwidth}{FST4}

5）插入“dogs”

与前一个单词“dog”进行最大前缀匹配，发现是dog，则在g后增加新边s，s边指向终点，如图\ref{FST5}所示：

\pic[!ht]{插入“dogs”}{width=0.6\textwidth}{FST5}

最终我们得到了如上一个有向无环图。利用该结构可以很方便的进行查询，如给定一个term“dog”，我们可以通过上述结构很方便的查询该term存不存在，甚至我们在构建过程中可以将单词与某一数字、单词进行关联，从而实现key-value的映射。

3、FST使用与性能评测

我们可以将FST当成一种Key-Value数据结构来进行使用，特别是在对内存开销要求较少的应用场景。

FST压缩率一般在3\~20倍之间，相对于TreeMap/HashMap的膨胀3倍，内存节省就有9倍到60倍，那么FST在性能方面的表现如何。
下面是在mac book（i7处理器）进行的简单测试，性能虽不如TreeMap和\\HashMap，但也算良好，能够满足大部分应用的需求，测试结果如表\ref{FST6}所示：

\pictable[htbp]{FST性能比较}{width=0.8\textwidth}{FST6}

全文检索的方式可以提升搜索的效率，但同时也增加了索引的时间开销，在小数据量的搜索时，总体的耗时可能和顺序扫描差不多，而当数据量很大的时候创建索引的时间也不低，但建立索引毕竟是个一次性过程，不需要每次搜索的时候都建立索引，只要是建立好了索引，以后就可以享受，这也是全文搜索的一个巨大优点。

\subsection{Lucene索引结构}
在Lucene中的检索是一种索引检索，使用更多的空间消耗来节省时间消耗，对需要搜索的文件和字符流进行全文索引，在搜索时，通过对索引快速检索，得到检索的位置，该位置为记录关键词出现的文件的路径或者某个关键词\citeup{8}。

Lucene中存储全文索引的结构为层次结构，共分5个层次：索引、段、文档、域和项，他们之间的关系如图\ref{image007}所示：

\pic[htbp]{Lucene索引存储结构概念图}{width=0.8\textwidth}{image007}

1.索引（Index）

每个目录都是一个索引，Lucene中的索引是放在文件夹中，如图2.5所示，同一个文件夹中的所有文件构成了一个索引。

2.段（Segment）

每个索引都包含了若干个段，每个段之间都是独立的，添加一个新的文档会生成一个新的段，不同的段之间可以合并。如图\ref{image008}所示，同一个段中的文件应该具有相同的前缀，图中共有"\_1"、"\_2"和"\_3"三个段。segments.gen和segments\_4是段的元数据文件，它们保存着段的属性信息。

3.文档（Document）

建立索引的基本单位就是文档，不同的段保存了不同的文档，一个段可以包含若干篇文档。新添加的文档单独保存在一个新生成的段中，随着段合并的过程，不同的文档会合并到同一个段中。

4.域（Field）

如果一篇文档中包含了不同类型的信息，我们可以分开对它进行索引，比如内容，作者，标题，时间等，这些可以在不同的域里保存，并且这些域的索引方式可以不同。

5.项（Term）

索引的最小单位即为项，项是经过了词法分析和语言处理后的字符串。


图\ref{image008}是Lucene生成索引的一个简单实例。Lucene的索引结构中，同时保存了正向信息和反向信息。

\pic[htbp]{Lucene索引实例}{width=0.8\textwidth}{image008}

正向信息：
按照一定的层次保存了从索引到项的关系：索引$\rightarrow$段$\rightarrow$文档$\rightarrow$域\\$\rightarrow$项，也就是说XXX索引包含了哪些段，XXX段又包含了哪些文档，XXX文档又包含了哪些域，XXX域又包含了哪些项。

在这样的层次结构中，每个层次都会保存着本层次的信息以及下一层次中的属性信息，比如一本介绍宇宙的书，首先会介绍宇宙中有可观测宇宙空间和未知宇宙空间，再介绍可观测宇宙空间中有着各种不同的星系，每个星系又包含了各种不同的星体，再介绍各个星体的特征之类的。

包含正向信息的文件有：

1. segments\_N，segments.gen保存着索引中包含了多少个段，每个段中又包含了多少篇文档。

2. AAA.fnm保存着这个段中包含了多少个域，每个域的名称和其索引方式。

3. AAA.fdx，AAA.fdt保存着这个段中包含的一系列文档，每个文档中包含多少个域，每个域中保存着哪些信息。

反向信息：保存了词典到倒排表的映射：项$\rightarrow$文档
包含反向信息的文件有：

1. AAA.tis，AAA.tii保存着词典：这个段包含的所有的词按照词典指定顺序的排序。

2. AAA.frq保存了倒排表：包含了每个项中文档的ID列表。

3. AAA.prx保存了倒排表中的每个项在其所对应的文档中的位置。

\subsection{Lucene搜索过程}
在本系统中，搜索的过程分成下列4个步骤：用户输入搜索关键词、处理关键词生成Query实例、使用IndexSearcher实例进行搜索、对返回的结果进行处理。
在用户输入搜索关键后，根据具体的设定，使用Query类中的某些查询类型的具体子类或QueryParser来进行处理，并且生成Query实例。
随后Query实例将会被传递给IndexSearcher的search方法中，进行搜索的过程，方法返回的结果会由TopDocs类的实例保存，使用ScoreDoc类实例来提供对TopDocs中每条结果的访问接口。


\subsection{模糊搜索}
要谈到模糊搜索就不得不提起与之对应的一个词“精确搜索”，下面简单介绍一下这两个词语的概念。
假定搜索的关键词是由若干个字组成的词组或短语。

精确检索指的是输入的关键词在检索结果中的字间顺序与字间间隔完全一样，比如搜索“电子科大”
出现的结果可能为：

1 “电子科大XXX”

2 “XXX电子科大”

3 “XXX电子科大XXX”

而模糊检索指的是输入的关键词在检索结果中可以近似出现，字符之间的顺序、字符之间的间隔可以产生变化，同样搜索“电子科大”出现的结果可能大多数都是：“XXX电子科技大学XXX”，结果中关键词“电子科大”被拆分。模糊搜索的好处是可以扩大搜索范围以及搜索结果的条目数，使用者可能会从相关结果中找到自己想要的结果。

下面介绍一种算法Levenshtein Distance（LD）即编辑距离算法\citeup{12}，用以实现系统中模糊搜索的功能。
该算法用于判断两个字符串的距离，或者叫模糊度。简单点理解就是差异程度。而这个差异程度的标准就是下列这些操作都会使距离值增加：

1 加一个字母(Insert),

2 删一个字母(Delete),

3 改变一个字母(Substitute)。

比如搜索的目标字符串为“word”，而源字符串为“world”，“world”与“word”之间的LD距离即为1，而“word”与“wild”之间的LD距离为2。

\subsection{中文分词}
对于Lucene查询来说，分析的操作会出现在两个地方：建立索引的时候和使用QueryParser构建Query对象实例的时候。如果在搜索结果中要以高亮的形式显示搜索内容，也可能会用到分析操作。如2.1.1提到的，分析操作被封装在分析器中。在本系统中，分析器主要工作是中文分词\citeup{6}。
下面简单介绍一下3款智能分词工具。ICTCLAS 是一款基于隐马尔科夫模型(HMM)的智能分词工具，而imdict-chinese-analyzer 和 ictclas4j 都是基于同一模型开发的Java版分词开源分词工具。


三种智能分词工具的效率对比
三者的分词效率对比如图\ref{image011}所示
\pic[htbp]{智能分词效率对比}{}{image011}

具体数据参见表\ref{tab1}：
\threelinetable[htpb]{tab1}{0.8\textwidth}{lccc}{具体数据对比}{ &ictclas4j&smart-chinese-analyzer&ICTCLAS 3.0\\}{
	分词速度(字节/秒)&14.75&491.44&661.54\\
	分词速度(汉字/秒)&7822&261235&348751\\
}{}

测试的数据为中文文件，大小为65432KB，内容长度为35126747字符，他们各自独立进行分词工作，并将分开的词语写到文件里。
测试环境均相同为：I5 CPU，内存4G。

由图\ref{image011}、表\ref{tab1}对比可以看出， smart-chinese-analyzer在分词的效率上与\\ICTCLAS的分词效率是同一个数量级的，为 ictclas4j 的 30多倍。
本系统拟采用smart-chinese-analyzer作为分析器。主要原因是其开源免费且具有良好的性能。smart-chinese-analyzer具有如下特性：

1. 完全支持Unicode：分词核心模块采用Unicode编码，对汉字的编码无需转换，提升了分词的效率。

2. 提升了搜索的效率：在智能词典的实践中，如果有智能中文分词时，索引文件比没有中文分词的文件小了接近30\%。

3. 提高了搜索准确度：smart-chinese-analyzer基于HHMM分词模型，极大提高了分词的准确率，在此基础上进行的搜索，比对汉字的逐个切分要更加准确。

4. 高效的数据结构：为了提高效率，针对常用中文检索的应用场景，smart-chinese-analyzer对功能进行了优化，例如人名识别、词性标注、时间识别等。另外还修改了原算法的数据结构，在内存占用量缩减到30\%的情况下把效率提升了数倍。

\section{Struts2框架}
\subsection{Struts2简介}
Struts 2是基于 struts 和WebWork的技术基础，进行了合并之后的全新框架。它的体系结构与Struts 1差别巨大。Struts 2以WebWork为核心，采用拦截器机制来处理用户的Request请求，这样的设计方式也使得业务逻辑控制器能够与Servlet API完全分离，所以Struts 2可以理解成新一代的WebWork产品。所有Struts 2程序都是基于client/server的HTTP交换协议，同样提供了MVC模式的清晰实现，包含了拦截器，OGNL、堆栈等参与对请求进行处理的组件。

\pic[htbp]{Struts2框架结构图}{width=0.4\textwidth}{image014}

\subsection{Struts2工作原理}

如图\ref{image014}所示为一个请求在Struts2框架中的处理过程，大致分成以下几步：

1. 客户端初始化一个指向Servlet容器（例如Tomcat）的请求。

2. 请求经过一系列过滤器（Filter）。

3. FilterDispatcher被调用，FilterDispatcher询问ActionMapper该请求需不需要调用某个Action。FilterDispatcher是控制器的核心，也就是mvc中控制层c的核心。

4. 如果ActionMapper确定需要调用某个Action，FilterDispatcher就会把请求交给ActionProxy来处理。

5. ActionProxy通过Configuration Manager访问框架中的Config文件，找到需要调用的Action类。

6. 同时ActionProxy会创建一个ActionInvocation实例。 

7. ActionInvocation实例使用Java命名模式来调用，在调用Action的时候，会涉及到相关拦截器（Intercepter）的调用。
 
8. Action执行完之后，ActionInvocation会从配置文件struts.xml中找到相应的返回结果。返回结果常常是一个需要被表示的FreeMarker或者JSP的模版。在表示的过程中会涉及使用Struts2 框架中继承的标签，以及ActionMapper。


\section{其他技术简介}
\subsection{Mybatis简介}
MyBatis是支持普通 SQL查询，存储过程和高级映射的优秀持久层框架。My\-Batis消除了几乎所有的 JDBC 代码和参数的手工设置以及结果集的检索。My\-Batis使用简单的 XML或注解用于配置和原始映射，将接口和 Java的POJOs（Plain Old Java Objects，普通的Java对象）映射成数据库中的记录。

相对Hibernate和Apache OJB等“一站式”ORM解决方案而言，MyBatis是一种“半自动化”的ORM实现。MyBatis需要开发人员自己来写SQL语句，这可以增加了程序的灵活性，在一定程度上可以作为ORM的一种补充。程序设计人员应该结合自己的项目的实际情况，来选择使用不同的策略。MyBatis和Hibernate都做了映射，但MyBatis是把实体类和SQL语句之间建立了映射关系，这种策略可以允许开发人员自己来写合适的SQL语句，而Hibernate在实体类和数据库之间建立了映射关系，SQL对于开发人员是不可见的，对于那些数据量非常大的应用，无法去优化SQL语句。

在本系统中，Mybatis框架完成了所有持久层的任务。
\subsection{Apache Commons Net简介}
Apache Commons Net™ 工具包实现了多种基本的网络协议的客户端。Apache Commons Net™ 工具包的目标是提供基本的协议访问，而非高级的抽象。因此某些设计违背了面向对象设计的原则。Apache Commons Net™ 工具包的理念是尽可能让一个协议的总体功能易于使用，同时在适用情况下提供基本协议的接口以便程序员构造自己定制的实现。

在FTP搜索引擎系统中，Apache Commons Net™ 工具包帮助系统完成了与FTP
服务器的全部交互工作。
\subsection{JDOM简介}
JDOM是一种特殊的Java开发工具包，使用了 XML ，用于提高开发 XML 应用程序的效率。它的设计中包含了 Java 的语法和语义。

JDOM是Brett Mclaughlin 和 Jason Hunter 的研发成果，他们是两位接触的 Java 开发人员， 2000 年初在类似 Apache 协议的许可下，JDOM作为一个开源项目正式启动。时至今日，JDOM已经发展壮大，成为一个成熟的系统，每天接收着众多优秀的 Java 开发人员的改进、集中反馈及BUG修复，并致力建立一个基于 Java 平台的完整解决方案，通过 Java 接口来进行访问、操作并输出 XML 数据。

在FTP搜索引擎系统中，JDOM完成对系统配置XML文档的相关操作。

